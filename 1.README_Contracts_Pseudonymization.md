# Pseudonymization Tool

Cet outil de pseudonymization est conçu pour remplacer les noms de personnes et d'organisations dans un texte tout en respectant certaines règles, comme ne pas pseudonymiser les figures historiques, les personnages fictifs ou les authorités publiques. L'outil utilise le modèle bert-base-cased.

## Notebooks
- Un notebook "Copilex_prototype" contenant uniquement l'application
- Un notebook "Copilex_projet" contenant toutes les étapes (modèle, fine-tuning, évaluation,...)

## Utilisation (Copilex_prototype)
### Structure des fichiers
- Entrée : Placez le texte brut à pseudonymiser dans un fichier .txt et indiquez-le dans input_file_path (au début du notebook)
- Sortie : Le texte pseudonymisé sera enregistré automatiquement dans un fichier nommé output_pseudonymized.txt

## Prérequis
### Environnement Python
Python 3.7 ou version supérieure

### Bibliothèques Python
- transformers
- request

## Dossier modèle fine-tuné
Lien google drive : https://drive.google.com/drive/folders/18jscAf_FYtQbtSVmuiVeUb9E665aOzKD?usp=drive_link

## Approche et Résultats (Copilex_projet)
### Choix du modèle
J'ai choisi le modèle bert-base-cased pour plusieurs raisons, notamment :
- Le dataset d'entraînement donné pour ce projet est en anglais
- Il est efficace de fine-tuner ce modèle
- Il a démontré de bonnes performances sur des benchmarks NER (Named Entity Recognition)

### Prétraitement des données
Pour les tâches de NER, le format IOB (Inside-Outside-Beginning) est essentiel, d'où la conception d'une fonction qui convertit les tokens en format IOB.
Aussi, deux fonctions permettent de convertir les labels (IOB) en indexes et inversement.
Le dataset a donc été traité selon ces fonctions, puis découpé en train, validation et test sets.

### Première évaluation avant le fine-tuning
Les métriques avant fine-tuning sont : {'precision': 0.8498823039500508, 'recall': 0.28026758369124877, 'f1': 0.40919325073870266}
Analyse :
- precision: la valeur élevée indique que le modèle pré-entraîné a déjà une certaine capacité à identifier des entités dans les données (grâce à son entraînement initial sur des tâches similaires)
- recall (proportion d'entités correctes dans le texte qui ont été détectées par le modèle) : la valeur faible indique que le modèle passe à côté d'une grande partie des entités présentes dans le texte, il produit beaucoup de faux négatifs (des entités existantes dans le texte qu'il ne reconnaît pas).
- F1-score (moyenne harmonique de la précision et du rappel, soit une mesure équilibrée des performances globales): le faible rappel tire le F1-score vers le bas, malgré une précision élevée. Cela montre que le modèle n'est pas encore efficace dans cette tâche spécifique.

### Fine-tuning
Fine-tuning du modèle avec Hugging Face (TrainingArguments, Trainer).
Les paramètres du modèle fine-tuné sont enregistrés dans un dossier qui est joint sur ce repository.

### Test de prédictions avec le modèle fine-tuné
Exemple : "One time, John Smith worked at Coca Cola and he drank from the Supreme Court."

Tokens : ['[CLS]', 'One', 'time', ',', 'John', 'Smith', 'worked', 'at', 'Coca', 'Cola', 'and', 'he', 'drank', 'from', 'the', 'Supreme', 'Court', '.', '[SEP]']

Labels : ['O', 'O', 'O', 'O', 'B-PERSON_NAME', 'I-PERSON_NAME', 'O', 'O', 'B-ORGANIZATION_NAME', 'I-ORGANIZATION_NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']

Avec cette petite phrase d'exemple, on voit que le modèle parvient correctement à identifier les noms et organisations, en laissant "Supreme Court" tel quel ('O' outside) car certes c'est une ORGANIZATION_NAME, mais c'est une authorité publique, donc à ne pas pseudonymiser.

### Evaluation du modèle fine-tuné
Les métriques après fine-tuning sont : {'precision': 0.9826046957472239, 'recall': 0.9828965921605813, 'f1': 0.982715197383457}\
Ces métriques montrent d'excellentes performances sur la tâche de NER.

Une autre approche avec Trainer permet d'obtenir un peu plus de détails :
eval_results =\
{'eval_loss': 0.054639969021081924,\
'eval_model_preparation_time': 0.0054,\
'eval_precision': 0.9823388085251653,\
'eval_recall': 0.9826839249770052,\
'eval_f1': 0.982455596035671,\
'eval_accuracy': 0.9826839249770052,\
'eval_runtime': 216.6362,\
'eval_samples_per_second': 35.197,\
'eval_steps_per_second': 2.202}\

# Pseudonymisation
On a vu, notamment avec la phrase d'exemple, que le modèle arrive bien à identifier les entités nommées. Maintenant, il faut en pseudonymiser certaines.\
Pour rappel, il faut pseudonymiser les PERSON_NAME et ORGANIZATION_NAME, mais pas les figures historiques, personnages fictifs, titres (Mr.), et les authorités publiques.
Le cas des authorités publiques fonctionne déjà comme le montre la non-pseudonymisation de "Supreme Court".\
Concentrons-nous donc sur les figures historiques et personnages fictifs :\
Une idée est d'aller interroger une base de données pour déterminer la nature des PERSON_NAME identifiés par le modèle, par exemple l'API Wikidata.
La fonction, conçue à cet effet, se rend sur l'url Wikidata associé au PERSON_NAME extrait et retourne True si ce nom contient des éléments dans l'élément "search" sur ce site, et False sinon.\
Ensuite, une fonction pseudonymise ces labels (PERSON_NAME et ORGANIZATION_NAME), que si PERSON_NAME n'a pas trouvé d'éléments dans Wikidata, ce qui indique que ce n'est pas un personnage historique et fictif.

Finalement, la fonction, prenant en argument un texte brut, retourne un fichier output_pseudonymized.txt contenant le texte pseudonymisé.
Voici un exemple :

test_text = """Faustine Laroche, CEO of Gatewai SAS, met with officials from the Supreme Court.\
She discussed plans with César Dupont from Coca Cola about a possible merger.\
Then, Faustine Laroche went home. And Coca Cola went down.\
Albert Einstein got a call from Robert Oppenheimer."""

La fonction appliquée à ce texte renvoie :\
Pseudonymized text: \
[PERSON_NAME_1], CEO of [ORGANIZATION_NAME_1], met with officials from the Supreme Court.\
She discussed plans with [PERSON_NAME_2] from [ORGANIZATION_NAME_2] about a possible merger.\
Then, [PERSON_NAME_1] went home. And [ORGANIZATION_NAME_2] went down.\
Albert Einstein got a call from Robert Oppenheimer.

Analyse:
Nous voyons que les noms Faustine Laroche et César Dupont ont bien été pseudonymisés (noms de particuliers par exemple), ainsi que les organisations Gatewai SAS et Coca Cola.\
On voit de plus que la deuxième mention d'une même étiquette reste sous le même format ("[PERSON_NAME_1]" par exemple reste constant tout le texte).\
Enfin, les noms Albert Einstein et Robert Oppenheimer ne sont pas pseudonymisés comme ce sont des figures historiques.

## Amélioration du système

### Modèle

D'autres méthodes pourraient être utilisées comme spacy ou stanza, spécifiques en NER.\
Aussi, si le texte n'est pas qu'en anglais, il faut utiliser un modèle multilingue, comme mBERT, et bert-base-multilingual-cased par exemple.\
Roberta-base ou roberta-large ont une meilleure compréhension du contexte, ce qui pourrait améliorer le traitement de cette tâche de NER.\
SpanBERT-large-cased est aussi une alternative, ce modèle étant conçu spécifiquement pour les tâches de NER et de relation entre entités avec d'excellente performance pour identifier et comprendre des entités liées. Ou dslim/bert-base-NER qui est spécialement fine-tuné pour les tâches NER (anglais).

### AUTHORITY_NAME

Les AUTHORITY_NAME ne sont pas traités dans ce code car ils ne pas présents dans les "spans" donc n'ont pas pu être identifiés et traités. Ils étaient présents au sein même du texte sous la forme : "texte...[AUTHORITY_NAME: ...suite du texte''.

### PSEUDONYMISATION

La méthode avec Wikidata n'est du tout optimale pour plusieurs raisons :

- appeler une API externe peut introduire des délais dans le traitement, et cela dépend de la disponibilité de l'API. Si l'API de Wikidata est en panne ou lente, cela affectera les performances du système.
- dépendance à une connexion internet
- Wikidata pourrait confondre certaines personnes historiques avec des homonymes
- cette base de données n'est pas adaptée à notre tâche. En effet, des personnalités publiques d'aujourd'hui devraient être pseudonymisées. Or, elles sont connues donc présentes sur Wikidata, donc le critère de pseudonymiser un nom que s'il n'est pas présent sur Wikidata ne marche plus.

Possible amélioration : inclure l'âge. Si la personne est morte depuis 50 ans par exemple (durée à déterminer), alors on la considère comme figures historiques. Ainsi, si le texte évoque par exemple "Elon Musk", qui est bien sûr présent dans la base Wikidata, alors avec le critère vivant/mort, ce nom sera pseudonymisé.

Autre amélioration possible : créer un dataset annoté des personnages historiques et fictifs.

Sinon, trouver une autre base de données spécialisée.
Ou utiliser un modèle qui a été entraîné sur un corpus où ces noms apparaissent fréquemment.


## Rapport de temps passé
Analyse du problème et réalisation du fine-tuning : un après-midi\
Analyse des résultats du fine-tuning et essais de pseudonymization : une demi-journée




